## 决策树 数学模型及推导

给定训练数据
$$x_i \in R^n, i=1,…,l$$  
以及标签好的y 
$$y \in R^l$$, 
A **decision tree** recursively partitions the space such that the samples with the same labels are grouped together. 
决策树递归的将这些数据和对应的标签划分到一起，递归的建树。


### ID3 算法
在这里，我们的输入是m个样本，实际上是m行的table，每个样本有n个离散特征，也就是n列，特征集合为A，输出为决策树T。
算法过程如下：
1. 初始化信息增益的阈值 $$\epsilon$$
2. 判断样本是否为同一类输出 $$D_i$$ (有没有到叶节点)，如果是则return 单节点树T，标记为$$D_i$$
3. 判断当前特征 A 是否为空，如果是则 return 单节点树 T，此时没有特征可以分了，所以标记样本输出类别D实例数最多的类别。
4. 计算A中的各个特征 （一共n个）对输出D的信息增益，选择信息增益最大的特征$$A_g$$
5. 判断$$A_g$$的信息增益小于阈值$$\epsilon$$, 则返回单节点T，标记类别为样本输出类别D实例最多的的类别，同3
6. 否则，按照特征$$A_g$$的不同取值$$A_{gi}$$将对应的样本输出D分成不同的类别$$D_i$$。每个类别产生一个子节点。对应特征值为$$A_{gi}$$。返回增加了节点的数T
7. 对于所有子节点，另 $$D = D_i$$ , $$A = A - \{A_g\}$$,递归调用 2-6步 (相当于往深一层继续建，并删去了已用的一个特征) 



#### ID3的不足
- 没有考虑连续特征，长度，密度等连续特征值没有办法在ID3里用 (Iris example)，基于离散特征
- ID3 采用沿信息增益大的特征简历决策树的节点，也就是说熵越高 (取值可能性比较多的特征)会比取值少的特征优先，然而他们可能都是完全不确定的




