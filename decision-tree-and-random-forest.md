## Decision Tree

Decision Tree 决策树 是一种树型结构，每个内部节点表示在一个属性上的测试，每个分支表示测试输出，每个叶节点代表的是一种类别。

决策树学习是自顶向下的递归方法，他是以信息熵为度量构造一棵熵值下降最快的树，到叶子结点处的熵值为0，此时每个叶节点中的实例都属于同一类。

决策树学习算法不需要使用者了解过多背景知识，只需要对训练实例进行较好的标注，就能够进行学习；属于有监督学习（supervised learning）
可以从一类无序，无规则的事物（概念）中推理出决策树表示的分类规则。

## 决策树生成算法

建立决策树的关键，就是在当前状态下选择哪个属性作为分类依据。

*信息增益* 表示得知特征属性A的信息而使得类X的信息的不确定性减少的程度。

特征A对训练数据集D的信息增益 g(D,A) = H(D) - H(D|A)

 H(D)表示数据集的经验熵(Entropy), H(D|A) 是特征A给定条件下数据集的经验熵。
 
 那么我们假设|D|代表数据集样本个数， 同时有K个类$$C_k$$， k = 1,2,... K; |$$C_k$$| 表示属于类Ck的样本个数，那么 $$\sum_{k} |C_k| = |D|$$
 
 我们可以把经验熵写成： 
 
 $$H(D) = -\sum_{k=1}^{K} \frac{|C_k|}{|D|}\log \frac{|C_k|}{|D|}$$
 
 
那么一般来说我们希望遍历所有特征，对于特征A：
我们计算A的信息增益
最后选择信息增益最大的特征作为当前的分裂特征。


